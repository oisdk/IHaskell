
@article{fredman_pairing_1986,
  title = {The Pairing Heap: {{A}} New Form of Self-Adjusting Heap},
  volume = {1},
  issn = {0178-4617, 1432-0541},
  shorttitle = {The Pairing Heap},
  doi = {10.1007/BF01840439},
  abstract = {Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue) called theFibonacci heap. Although theoretically efficient, Fibonacci heaps are complicated to implement and not as fast in practice as other kinds of heaps. In this paper we describe a new form of heap, called thepairing heap, intended to be competitive with the Fibonacci heap in theory and easy to implement and fast in practice. We provide a partial complexity analysis of pairing heaps. Complete analysis remains an open problem.},
  language = {en},
  number = {1-4},
  journal = {Algorithmica},
  author = {Fredman, Michael L. and Sedgewick, Robert and Sleator, Daniel D. and Tarjan, Robert E.},
  month = jan,
  year = {1986},
  pages = {111-129},
  file = {/Users/doisinkidney/Zotero/storage/3B5VZMSF/pairingheaps.pdf;/Users/doisinkidney/Zotero/storage/P3ZFSZ4B/10.html}
}

@inproceedings{du_pin_calmon_privacy_2012,
  title = {Privacy against Statistical Inference},
  booktitle = {Communication, {{Control}}, and {{Computing}} ({{Allerton}}), 2012 50th {{Annual Allerton Conference}} On},
  publisher = {{IEEE}},
  author = {{du Pin Calmon}, Fl{\'a}vio and Fawaz, Nadia},
  year = {2012},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Information Theory},
  pages = {1401--1408},
  file = {/Users/doisinkidney/Zotero/storage/N2VCXT6I/Calmon and Fawaz - 2012 - Privacy Against Statistical Inference.pdf;/Users/doisinkidney/Zotero/storage/QAP3XTNT/du Pin Calmon and Fawaz - 2012 - Privacy against statistical inference.pdf;/Users/doisinkidney/Zotero/storage/EZRVUF58/1210.html;/Users/doisinkidney/Zotero/storage/HM3224C6/6483382.html},
  annote = {Comment: Allerton 2012, 8 pages}
}

@article{calmon_principal_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00820},
  primaryClass = {cs, math},
  title = {Principal {{Inertia Components}} and {{Applications}}},
  abstract = {We explore properties and applications of the Principal Inertia Components (PICs) between two discrete random variables \$X\$ and \$Y\$. The PICs lie in the intersection of information and estimation theory, and provide a fine-grained decomposition of the dependence between \$X\$ and \$Y\$. Moreover, the PICs describe which functions of \$X\$ can or cannot be reliably inferred (in terms of MMSE) given an observation of \$Y\$. We demonstrate that the PICs play an important role in information theory, and they can be used to characterize information-theoretic limits of certain estimation problems. In privacy settings, we prove that the PICs are related to fundamental limits of perfect privacy.},
  journal = {arXiv:1704.00820 [cs, math]},
  author = {Calmon, Flavio P. and Makhdoumi, Ali and M{\'e}dard, Muriel and Varia, Mayank and Christiansen, Mark and Duffy, Ken R.},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Information Theory},
  file = {/Users/doisinkidney/Zotero/storage/EEE87PI9/Calmon et al. - 2017 - Principal Inertia Components and Applications.pdf;/Users/doisinkidney/Zotero/storage/QTPCS2JA/1704.html},
  annote = {Comment: Overlaps with arXiv:1405.1472 and arXiv:1310.1512}
}

@inproceedings{naor_anti-persistence_2001,
  address = {New York, NY, USA},
  series = {STOC '01},
  title = {Anti-Persistence: {{History Independent Data Structures}}},
  isbn = {978-1-58113-349-3},
  shorttitle = {Anti-Persistence},
  doi = {10.1145/380752.380844},
  abstract = {Many data structures give away much more information than they were intended to. Whenever privacy is important, we need to be concerned that it might be possible to infer information from the memory representation of a data structure that is not available through its ``legitimate'' interface. Word processors that quietly maintain old versions of a document are merely the most egregious example of a general problem.We deal with data structures whose current memory representation does not reveal their history. We focus on dictionaries, where this means revealing nothing about the order of insertions or deletions. Our first algorithm is a hash table based on open addressing, allowing O(1) insertion and search.  We also present a history independent dynamic  perfect hash table that uses space linear in the number of elements inserted and has expected amortized insertion and deletion time O(1).  To solve the dynamic perfect hashing problem we devise a general scheme for history independent memory allocation. For fixed-size records this is quite efficient, with insertion and deletion both linear in the size of the record.  Our variable-size record scheme is efficient enough for dynamic perfect hashing but not for general use.  The main open problem we leave is whether it is possible to implement a variable-size record scheme with low overhead.},
  booktitle = {Proceedings of the {{Thirty}}-Third {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Naor, Moni and Teague, Vanessa},
  year = {2001},
  keywords = {algorithms,anti-persistence,data structures,hash table,history independence,privacy,security},
  pages = {492--501},
  file = {/Users/doisinkidney/Zotero/storage/3B3M2Z4Y/Naor and Teague - 2001 - Anti-persistence History Independent Data Structu.pdf;/Users/doisinkidney/Zotero/storage/556UUJ3A/Naor and Teague - 2001 - Anti-persistence History independent data structu.pdf;/Users/doisinkidney/Zotero/storage/4BEGEYMV/summary.html}
}

@phdthesis{golovin_uniquely_2008,
  address = {Pittsburgh, PA, USA},
  type = {{{PhD Thesis}}},
  title = {Uniquely {{Represented Data Structures}} with {{Applications}} to {{Privacy}}},
  abstract = {In a typical application storing some data, if the memory representations of the internal data structures are inspected, they may leave significant clues to the past use of the application. For example, a data structure with lazy deletions might retain an object that the user believes was deleted long ago; this is problematic in environments requiring high security or strict privacy guarantees. We can eliminate such problems entirely by demanding that a data structure implementation store exactly the information specified by an abstract data type (ADT), and nothing more. This property is sometimes called strong history independence. To attain it, it is often necessary and always sufficient to ensure the data structure is uniquely represented. That is, any two sequences of operations which bring the ADT to the same logical state will cause the implementation to generate the same memory representation. This observation begs the following question. For each abstract data type, what is the added cost for uniquely represented implementations over their conventional counterparts, in terms of time, space, and randomness? In this dissertation, we will answer this question for several important abstract data types, and argue that the overhead for unique representation is sufficiently low to warrant its widespread use in high security and high privacy environments. Towards this end, we provide the theoretical foundation for the development of efficient uniquely represented systems that provably store exactly the information their designs specify, and nothing more.},
  school = {Carnegie Mellon University},
  author = {Golovin, Daniel},
  year = {2008},
  file = {/Users/doisinkidney/Zotero/storage/RMQBGV9J/Golovin - 2008 - Uniquely Represented Data Structures with Applicat.pdf;/Users/doisinkidney/Zotero/storage/VE585VI7/Golovin - 2008 - Uniquely Represented Data Structures with Applicat.pdf},
  annote = {AAI3340637}
}

@inproceedings{hoogerwoord_logarithmic_1992,
  title = {A {{Logarithmic Implementation}} of {{Flexible Arrays}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Mathematics}} of {{Program Construction}}},
  publisher = {{Springer-Verlag}},
  author = {Hoogerwoord, Rob R.},
  year = {1992},
  pages = {191--207},
  file = {/Users/doisinkidney/Zotero/storage/9GWPDIBX/Hoogerwoord - A Logarithmic Implementation of Flexible Arrays.pdf;/Users/doisinkidney/Zotero/storage/BCPNRZXS/Hoogerwoord - 1992 - A logarithmic implementation of flexible arrays.pdf;/Users/doisinkidney/Zotero/storage/RKJIGB5K/Hoogerwoord - A Logarithmic Implementation of Flexible Arrays.pdf;/Users/doisinkidney/Zotero/storage/AHTTFRCS/3-540-56625-2_14.html;/Users/doisinkidney/Zotero/storage/GMA27KPG/bwmeta1.element.html;/Users/doisinkidney/Zotero/storage/KEF7QFR2/772185.html;/Users/doisinkidney/Zotero/storage/QSH92WBV/3-540-56625-2_14.html;/Users/doisinkidney/Zotero/storage/W5VHKZGY/oailibrary.tue.html;/Users/doisinkidney/Zotero/storage/XTULH7G2/citation.html}
}

@inproceedings{micciancio_oblivious_1997,
  address = {New York, NY, USA},
  series = {STOC '97},
  title = {Oblivious {{Data Structures}}: {{Applications}} to {{Cryptography}}},
  isbn = {978-0-89791-888-6},
  shorttitle = {Oblivious {{Data Structures}}},
  doi = {10.1145/258533.258638},
  booktitle = {Proceedings of the {{Twenty}}-Ninth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Micciancio, Daniele},
  year = {1997},
  pages = {456--464},
  file = {/Users/doisinkidney/Zotero/storage/4SPGGRV3/Micciancio - 1997 - Oblivious Data Structures Applications to Cryptog.pdf;/Users/doisinkidney/Zotero/storage/KHDXK7YK/Micciancio - 1997 - Oblivious Data Structures Applications to Cryptog.pdf}
}

@inproceedings{sundar_unique_1990,
  address = {New York, NY, USA},
  series = {STOC '90},
  title = {Unique {{Binary Search Tree Representations}} and {{Equality}}-Testing of {{Sets}} and {{Sequences}}},
  isbn = {978-0-89791-361-4},
  doi = {10.1145/100216.100219},
  booktitle = {Proceedings of the {{Twenty}}-Second {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Sundar, R. and Tarjan, R. E.},
  year = {1990},
  pages = {18--25},
  file = {/Users/doisinkidney/Zotero/storage/6HCHLXSS/Sundar and Tarjan - 1990 - Unique Binary Search Tree Representations and Equa.pdf}
}

@inproceedings{fung_top-down_2005,
  title = {Top-{{Down Specialization}} for {{Information}} and {{Privacy Preservation}}},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Data Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Fung, Benjamin and Wang, Ke and Yu, Philip S.},
  year = {2005},
  pages = {205--216},
  file = {/Users/doisinkidney/Zotero/storage/5RSRLDTG/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/C4G93M5Z/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/CNTRRGII/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/KXAUB96S/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/MXHQGXRQ/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/YGA7N4IG/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/Z5VCKZYQ/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/ZY8AAIFI/Fung et al. - Top-Down Specialization for Information and Privac.pdf;/Users/doisinkidney/Zotero/storage/C7SJ4TF6/summary.html;/Users/doisinkidney/Zotero/storage/ET3XSNMF/citation.html;/Users/doisinkidney/Zotero/storage/ZAV5UBK8/1410123.html}
}

@article{sweeney_achieving_2002,
  title = {{{ACHIEVING}} K-{{ANONYMITY PRIVACY PROTECTION USING GENERALIZATION}}  {{AND SUPPRESSION}}},
  volume = {10},
  issn = {0218-4885},
  doi = {10.1142/S021848850200165X},
  abstract = {Often a data holder, such as a hospital or bank, needs to share  person-specific records in such a way that the identities of  the individuals who are the subjects of the data cannot be determined.  One way to achieve this is to have the released records adhere to  k-anonymity, which means each released record has at least  (k-1) other records in the release whose values are indistinct  over those fields that appear in external data. So, k-anonymity  provides privacy protection by guaranteeing that each released record will  relate to at least k individuals even if the records are directly linked  to external information. This paper provides a formal presentation of combining  generalization and suppression to achieve k-anonymity. Generalization  involves replacing (or recoding) a value with a less specific but semantically  consistent value. Suppression involves not releasing a value at all. The  Preferred Minimal Generalization Algorithm (MinGen), which is a theoretical  algorithm presented herein, combines these techniques to provide  k-anonymity protection with minimal distortion. The  real-world algorithms Datafly and $\mu$-Argus are compared  to MinGen. Both Datafly and $\mu$-Argus use heuristics to make  approximations, and so, they do not always yield optimal results. It  is shown that Datafly can over distort data and $\mu$-Argus can  additionally fail to provide adequate protection.},
  number = {05},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  author = {Sweeney, Latanya},
  month = oct,
  year = {2002},
  pages = {571-588},
  file = {/Users/doisinkidney/Zotero/storage/MTBUX8ZJ/SWEENEY - ACHIEVING k-ANONYMITY PRIVACY PROTECTION USING GEN.pdf;/Users/doisinkidney/Zotero/storage/TP36X66W/SWEENEY - ACHIEVING k-ANONYMITY PRIVACY PROTECTION USING GEN.pdf;/Users/doisinkidney/Zotero/storage/YFXPAMQY/S021848850200165X.html}
}

@inproceedings{lefevre_mondrian_2006,
  title = {Mondrian Multidimensional K-Anonymity},
  abstract = {K-Anonymity has been proposed as a mechanism for protecting privacy in microdata publishing, and numerous recoding ``models '' have been considered for achieving kanonymity. This paper proposes a new multidimensional model, which provides an additional degree of flexibility not seen in previous (single-dimensional) approaches. Often this flexibility leads to higher-quality anonymizations, as measured both by general-purpose metrics and more specific notions of query answerability. Optimal multidimensional anonymization is NP-hard (like previous optimal k-anonymity problems). However, we introduce a simple greedy approximation algorithm, and experimental results show that this greedy algorithm frequently leads to more desirable anonymizations than exhaustive optimal algorithms for two single-dimensional models. 1.},
  booktitle = {In {{ICDE}}},
  author = {Lefevre, Kristen and Dewitt, David J. and Ramakrishnan, Raghu},
  year = {2006},
  file = {/Users/doisinkidney/Zotero/storage/DNGRB7B3/Lefevre et al. - 2006 - Mondrian multidimensional k-anonymity.pdf;/Users/doisinkidney/Zotero/storage/5J4RMJTN/summary.html}
}

@inproceedings{bayardo_data_2005,
  address = {Washington, DC, USA},
  series = {ICDE '05},
  title = {Data {{Privacy Through Optimal}} K-{{Anonymization}}},
  isbn = {978-0-7695-2285-2},
  doi = {10.1109/ICDE.2005.42},
  abstract = {Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimalk-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a nontrivial dataset under a general model of the problem.},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Data Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Bayardo, Roberto J. and Agrawal, Rakesh},
  year = {2005},
  pages = {217--228},
  file = {/Users/doisinkidney/Zotero/storage/ZLI9FVSD/Bayardo and Agrawal - 2005 - Data Privacy Through Optimal k-Anonymization.pdf}
}

@inproceedings{meyerson_complexity_2004,
  address = {New York, NY, USA},
  series = {PODS '04},
  title = {On the {{Complexity}} of {{Optimal K}}-Anonymity},
  isbn = {978-1-58113-858-0},
  doi = {10.1145/1055558.1055591},
  abstract = {The technique of k-anonymization has been proposed in the literature as an alternative way to release public information, while ensuring both data privacy and data integrity. We prove that two general versions of optimal k-anonymization of relations are NP-hard, including the suppression version which amounts to choosing a minimum number of entries to delete from the relation. We also present a polynomial time algorithm for optimal k-anonymity that achieves an approximation ratio independent of the size of the database, when k is constant. In particular, it is a O(k log k)-approximation where the constant in the big-O is no more than 4, However, the runtime of the algorithm is exponential in k. A slightly more clever algorithm removes this condition, but is a O(k log m)-approximation, where m is the degree of the relation. We believe this algorithm could potentially be quite fast in practice.},
  booktitle = {Proceedings of the {{Twenty}}-Third {{ACM SIGMOD}}-{{SIGACT}}-{{SIGART Symposium}} on {{Principles}} of {{Database Systems}}},
  publisher = {{ACM}},
  author = {Meyerson, Adam and Williams, Ryan},
  year = {2004},
  pages = {223--228},
  file = {/Users/doisinkidney/Zotero/storage/XG2JCR2N/Meyerson and Williams - 2004 - On the Complexity of Optimal K-anonymity.pdf}
}

@article{harada_k-anonymization_2010,
  title = {K-{{Anonymization}} Schemes with Automatic Generation of Generalization Trees and Distortion Measuring Using Information Entropy},
  journal = {IPSJ SIG Notes},
  author = {Harada, Kunihiko and Sato, Y.},
  year = {2010},
  pages = {1--7}
}

@incollection{andersson_fundamentals_2008,
  series = {IFIP \textemdash{} The International Federation for Information Processing},
  title = {On the {{Fundamentals}} of {{Anonymity Metrics}}},
  isbn = {978-1-4419-4629-4 978-0-387-79026-8},
  abstract = {In recent years, a handful of anonymity metrics have been proposed that are either based on (i) the number participants in the given scenario, (ii) the probability distribution in an anonymous network regarding which participant is the sender / receiver, or (iii) a combination thereof. In this paper, we discuss elementary properties of metrics in general and anonymity metrics in particular, and then evaluate the behavior of a set of state-of-the-art anonymity metrics when applied in a number of scenarios. On the basis of this evaluation and basic measurement theory, we also define criteria for anonymity metrics and show that none of the studied metrics fulfill all criteria. Lastly, based on previous work on entropy-based anonymity metrics, as well as on theories on the effective support size of the entropy function and on Huffman codes, we propose an alternative metric \textemdash{} the scaled anonymity set size \textemdash{} that fulfills these criteria.},
  language = {en},
  booktitle = {The {{Future}} of {{Identity}} in the {{Information Society}}},
  publisher = {{Springer, Boston, MA}},
  author = {Andersson, Christer and Lundin, Reine},
  year = {2008},
  pages = {325-341},
  file = {/Users/doisinkidney/Zotero/storage/GN9KNKC8/Andersson and Lundin - 2008 - On the Fundamentals of Anonymity Metrics.pdf;/Users/doisinkidney/Zotero/storage/8M99TG4B/10.html},
  doi = {10.1007/978-0-387-79026-8_23}
}

@inproceedings{park_approximate_2007,
  address = {New York, NY, USA},
  series = {SIGMOD '07},
  title = {Approximate {{Algorithms}} for {{K}}-Anonymity},
  isbn = {978-1-59593-686-8},
  doi = {10.1145/1247480.1247490},
  booktitle = {Proceedings of the 2007 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  author = {Park, Hyoungmin and Shim, Kyuseok},
  year = {2007},
  keywords = {anonymity,data mining,data publishing,local recoding,privacy preservation},
  pages = {67--78},
  file = {/Users/doisinkidney/Zotero/storage/5EL2QGIV/Park and Shim - 2007 - Approximate Algorithms for K-anonymity.pdf}
}

@inproceedings{lefevre_incognito_2005,
  address = {New York, NY, USA},
  series = {SIGMOD '05},
  title = {Incognito: {{Efficient Full}}-Domain {{K}}-Anonymity},
  isbn = {978-1-59593-060-6},
  shorttitle = {Incognito},
  doi = {10.1145/1066157.1066164},
  abstract = {A number of organizations publish microdata for purposes such as public health and demographic research. Although attributes that clearly identify individuals, such as Name and Social Security Number, are generally removed, these databases can sometimes be joined with other public databases on attributes such as Zipcode, Sex, and Birthdate to re-identify individuals who were supposed to remain anonymous. "Joining" attacks are made easier by the availability of other, complementary, databases over the Internet.K-anonymization is a technique that prevents joining attacks by generalizing and/or suppressing portions of the released microdata so that no individual can be uniquely distinguished from a group of size k. In this paper, we provide a practical framework for implementing one model of k-anonymization, called full-domain generalization. We introduce a set of algorithms for producing minimal full-domain generalizations, and show that these algorithms perform up to an order of magnitude faster than previous algorithms on two real-life databases.Besides full-domain generalization, numerous other models have also been proposed for k-anonymization. The second contribution in this paper is a single taxonomy that categorizes previous models and introduces some promising new alternatives.},
  booktitle = {Proceedings of the 2005 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
  year = {2005},
  pages = {49--60},
  file = {/Users/doisinkidney/Zotero/storage/PYG2PFK7/LeFevre et al. - 2005 - Incognito Efficient Full-domain K-anonymity.pdf}
}

@inproceedings{harada_reducing_2012,
  title = {Reducing {{Amount}} of {{Information Loss}} in K-{{Anonymization}} for {{Secondary Use}} of {{Collected Personal Information}}},
  doi = {10.1109/SRII.2012.18},
  abstract = {A lot of information has recently been collected and the need to put it to secondary use is expanding. This is because a lot of useful knowledge is contained in it. There are always privacy concerns with the secondary use of personal information. k-anonymization is a tool that enables us to release personal information in a manner that is privacy-protected. In classical k-anonymization, side information, which is termed generalization hierarchies, is always needed. In addition, the quality of k-anonymized data has always been a central problem in the area because information loss is an inherent feature of anonymization. This paper proposes a new scheme in which generalization hierarchies are automatically constructed by input information. This scheme not only contributes to reducing the cost of operations for preparing side information, but also to increasing the quality of k-anonymization results. Experiments have demonstrated that k-anonymization with automatically constructed hierarchies sacrifices 38\% less data (measured by information entropy) than that with complete binary trees (introduced as classically-used hierarchies).},
  booktitle = {2012 {{Annual SRII Global Conference}}},
  author = {Harada, K. and Sato, Y. and Togashi, Y.},
  month = jul,
  year = {2012},
  keywords = {data privacy,Privacy,Binary trees,Clustering algorithms,collected personal information,Information entropy,information loss,input information,k-anonymization,k-anonymized data,Measurement,Outsourcing,personal information,privacy protection,secondary use},
  pages = {61-69},
  file = {/Users/doisinkidney/Zotero/storage/RZD5X6ZU/6310982.html}
}

@article{atzori_anonymity_2008,
  title = {Anonymity {{Preserving Pattern Discovery}}},
  volume = {17},
  issn = {1066-8888},
  doi = {10.1007/s00778-006-0034-x},
  abstract = {It is generally believed that data mining results do not violate the anonymity of the individuals recorded in the source database. In fact, data mining models and patterns, in order to ensure a required statistical significance, represent a large number of individuals and thus conceal individual identities: this is the case of the minimum support threshold in frequent pattern mining. In this paper we show that this belief is ill-founded. By shifting the concept of k
-anonymity from the source data to the extracted patterns, we formally characterize the notion of a threat to anonymity in the context of pattern discovery, and provide a methodology to efficiently and effectively identify all such possible threats that arise from the disclosure of the set of extracted patterns. On this basis, we obtain a formal notion of privacy protection that allows the disclosure of the extracted knowledge while protecting the anonymity of the individuals in the source database. Moreover, in order to handle the cases where the threats to anonymity cannot be avoided, we study how to eliminate such threats by means of pattern (not data!) distortion performed in a controlled way.},
  number = {4},
  journal = {The VLDB Journal},
  author = {Atzori, Maurizio and Bonchi, Francesco and Giannotti, Fosca and Pedreschi, Dino},
  month = jul,
  year = {2008},
  keywords = {anonymity,Frequent pattern mining,Individual privacy,Knowledge discovery,Privacy preserving data mining},
  pages = {703--727},
  file = {/Users/doisinkidney/Zotero/storage/QAG6QVQU/Atzori et al. - 2008 - Anonymity Preserving Pattern Discovery.pdf}
}

@incollection{willenborg_application_2001,
  series = {Lecture Notes in Statistics},
  title = {Application of {{Non}}-{{Perturbative SDC Techniques}} for {{Microdata}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {The aim of the present chapter is to consider the problem of producing a safe microdata set by applying global recodings and local suppressions, as discussed in Chapter 1. In our discussion we assume disclosure scenarios of the type discussed in Chapter 2. These scenarios have in common that an intruder is supposed to use a number of low-dimensional combinations of key variables in an attempt to disclose private information. Global recoding and local suppression should be applied in such a way that this type of disclosure is prevented, or at least sufficiently hampered. This can be achieved by making sure unsafe combinations, i.e. with a frequencies below certain assumed threshold values, do not occur. This is precisely the case when the global recodings or local suppressions have yielded a safe microdata set (assuming the disclosure scenario adopted does apply) and with minimum information loss (using an information loss measure as discussed in Chapter 3). Clearly to obtain such a safe microdata through modification of the original unsafe microdata set requires an optimization problem to be solved.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {93-106},
  file = {/Users/doisinkidney/Zotero/storage/NBMPN9TL/Willenborg and Waal - 2001 - Application of Non-Perturbative SDC Techniques for.pdf;/Users/doisinkidney/Zotero/storage/MQ23W7FG/978-1-4613-0121-9_4.html},
  doi = {10.1007/978-1-4613-0121-9_4}
}

@incollection{willenborg_information_2001,
  series = {Lecture Notes in Statistics},
  title = {Information {{Loss}} in {{Tabular Data}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In the present chapter we discuss the impact of SDC techniques on the statistical quality of tables. This impact on the quality is subsumed under the heading ``information loss''.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {159-173},
  file = {/Users/doisinkidney/Zotero/storage/79Y5HG8F/Willenborg and Waal - 2001 - Information Loss in Tabular Data.pdf;/Users/doisinkidney/Zotero/storage/PL3QAIPN/978-1-4613-0121-9_7.html},
  doi = {10.1007/978-1-4613-0121-9_7}
}

@incollection{willenborg_data_2001,
  series = {Lecture Notes in Statistics},
  title = {Data {{Analytic Impact}} of {{SDC Techniques}} on {{Microdata}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {The aim of this chapter is to discuss the impact of SDC techniques on the data analytic potential of microdata. There is no single correct way to define ``analytic potential'' since different users might analyze a given set of microdata in different unforeseen ways. We shall begin by assuming that the purpose of the analysis is to estimate a specified set of population parameters. These might be descriptive parameters, such as means or proportions or they may be analytic parameters, such as the coefficients of a regression model. We consider the impact of SDC techniques on the estimation of these parameters and, specifically, the impact of the SDC techniques discussed in Chapter 1.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {71-91},
  file = {/Users/doisinkidney/Zotero/storage/F9TAFG3L/Willenborg and Waal - 2001 - Data Analytic Impact of SDC Techniques on Microdat.pdf;/Users/doisinkidney/Zotero/storage/RVZ7GDGX/978-1-4613-0121-9_3.html},
  doi = {10.1007/978-1-4613-0121-9_3}
}

@incollection{willenborg_application_2001-1,
  series = {Lecture Notes in Statistics},
  title = {Application of {{Perturbative Techniques}} for {{Tabular Data}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In this final chapter we consider the SDC techniques adding noise, rounding and source data perturbation (SDP). Adding noise perturbs the cell values in a table by adding random values to them. The random values are generated according to a prescribed probability distribution. ``Rounding'' in fact refers not to a particular SDC-technique but rather to a class of SDC-techniques. Each of these SDC-techniques has its own advantages and disadvantages.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {219-243},
  file = {/Users/doisinkidney/Zotero/storage/2JM4HHAY/Willenborg and Waal - 2001 - Application of Perturbative Techniques for Tabular.pdf;/Users/doisinkidney/Zotero/storage/RPTSLFIS/978-1-4613-0121-9_9.html},
  doi = {10.1007/978-1-4613-0121-9_9}
}

@incollection{willenborg_disclosure_2001,
  series = {Lecture Notes in Statistics},
  title = {Disclosure {{Risks}} for {{Microdata}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In this chapter we consider the potential disclosure risk arising from the release of microdata. We suppose that the data consist of a standard rectangular data file containing values of variables which at this stage have undergone no SDC treatment. We consider first possible scenarios by which an intruder might attempt to achieve disclosure. This enables us to specify a framework within which disclosure risk may be defined in terms of an intruder's predictive probability distribution for values of confidential variables. Following a discussion of this predictive approach to measuring disclosure risk, we present arguments for preferring instead to measure risk in terms of the probability of re-identification. The estimation of re-identification risk is discussed in general and for the important special case of discrete variables.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {39-70},
  file = {/Users/doisinkidney/Zotero/storage/V3KZK33J/Willenborg and Waal - 2001 - Disclosure Risks for Microdata.pdf;/Users/doisinkidney/Zotero/storage/M7JUAUAH/978-1-4613-0121-9_2.html},
  doi = {10.1007/978-1-4613-0121-9_2}
}

@incollection{willenborg_disclosure_2001-1,
  series = {Lecture Notes in Statistics},
  title = {Disclosure {{Risk}} for {{Tabular Data}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In this chapter we consider the assessment of disclosure risk for tabular data. Disclosure risk may be defined either for the whole table or separately for each cell into which the table is organized. We shall sometimes use the term sensitivity as an alternative term for the disclosure risk of a table or cell. We suppose that a threshold may be specified as the maximum value below which the disclosure risk is deemed acceptable. Disclosure risk exceeding the threshold will call for the use of some form of SDC technique. For a measure of disclosure risk defined at the table level, we say that the table is sensitive if the disclosure risk of the table exceeds the given threshold. For a measure of disclosure risk defined at the cell level, we similarly say that a cell is sensitive if its disclosure risk is greater than the given threshold. In this book we restrict ourselves to measures of disclosure risk defined at the cell level. The objective of disclosure risk assessment will then be to determine which cells of a table are sensitive. We assume that a table containing sensitive cells may not be published. Having identified which cells are sensitive, the next step will be to treat these cells with an SDC technique such as cell suppression. This will be discussed in Chapters 8 and 9.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {137-157},
  file = {/Users/doisinkidney/Zotero/storage/42JXUMDQ/Willenborg and Waal - 2001 - Disclosure Risk for Tabular Data.pdf;/Users/doisinkidney/Zotero/storage/2DUKJS4K/978-1-4613-0121-9_6.html},
  doi = {10.1007/978-1-4613-0121-9_6}
}

@incollection{willenborg_overview_2001,
  series = {Lecture Notes in Statistics},
  title = {Overview of the {{Area}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {Organizations conducting surveys and other forms of data collection may release the results of these exercises to third party users as ``statistical products'' in a variety of formats. For example, they may release tables to the public through published reports or release microdata files to academics for secondary data analysis. The problem addressed in statistical disclosure control (SDC) is that it is conceivable that a person who is given access to one of these statistical products may, through inappropriate use of the data, be able to disclose confidential information about the individual units which originally provided the data. These units might, for example, be respondents to a survey or persons completing forms for administrative purposes.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {1-37},
  file = {/Users/doisinkidney/Zotero/storage/C2IR6R63/Willenborg and Waal - 2001 - Overview of the Area.pdf;/Users/doisinkidney/Zotero/storage/JXQAMCJM/978-1-4613-0121-9_1.html},
  doi = {10.1007/978-1-4613-0121-9_1}
}

@incollection{willenborg_application_2001-2,
  series = {Lecture Notes in Statistics},
  title = {Application of {{Non}}-{{Perturbative Techniques}} for {{Tabular Data}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In the present chapter we discuss the application of two techniques to protect a table or a (hierarchical or linked) set of tables against disclosure, namely table redesign and secondary cell suppression. We start our discussion by considering a single table with its marginals, and then consider the more general case of hierarchical and linked tables. In fact the single table case is the one that has been given most attention in the literature, as it is the more basic one. Increasingly, however, attention is being paid to the hierarchical and linked table case. Since this generalization has not been settled at the time of writing we do not dwell very extensively on this subject.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {175-217},
  file = {/Users/doisinkidney/Zotero/storage/MTP3A633/Willenborg and Waal - 2001 - Application of Non-Perturbative Techniques for Tab.pdf;/Users/doisinkidney/Zotero/storage/JA6AHP82/978-1-4613-0121-9_8.html},
  doi = {10.1007/978-1-4613-0121-9_8}
}

@incollection{willenborg_application_2001-3,
  series = {Lecture Notes in Statistics},
  title = {Application of {{Perturbative SDC Techniques}} for {{Microdata}}},
  isbn = {978-0-387-95121-8 978-1-4613-0121-9},
  abstract = {In this chapter we consider the application of some perturbative techniques to produce safe microdata. As discussed in Chapter 2, the risk of disclosure is conceived of as arising from the possibility that an intruder matches the values of key variables in the microdata to corresponding values in prior information. The approach in this chapter is to perturb the values of potential key variables in the microdata so that they cannot be matched to external data sources so easily.},
  language = {en},
  booktitle = {Elements of {{Statistical Disclosure Control}}},
  publisher = {{Springer, New York, NY}},
  author = {Willenborg, Leon and de Waal, Ton},
  year = {2001},
  pages = {107-136},
  file = {/Users/doisinkidney/Zotero/storage/BSQ6LEEA/Willenborg and Waal - 2001 - Application of Perturbative SDC Techniques for Mic.pdf;/Users/doisinkidney/Zotero/storage/J6A35CFD/978-1-4613-0121-9_5.html},
  doi = {10.1007/978-1-4613-0121-9_5}
}

@inproceedings{machanavajjhala_l-diversity_2006,
  title = {L-Diversity: Privacy beyond k-Anonymity},
  shorttitle = {L-Diversity},
  doi = {10.1109/ICDE.2006.1},
  abstract = {Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called kappa-anonymity has gained popularity. In a kappa-anonymized dataset, each record is indistinguishable from at least k\textemdash{}1 other records with respect to certain "identifying" attributes. In this paper we show with two simple attacks that a kappa-anonymized dataset has some subtle, but severe privacy problems. First, we show that an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. Second, attackers often have background knowledge, and we show that kappa-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks and we propose a novel and powerful privacy definition called ell-diversity. In addition to building a formal foundation for ell-diversity, we show in an experimental evaluation that ell-diversity is practical and can be implemented efficiently.},
  booktitle = {22nd {{International Conference}} on {{Data Engineering}} ({{ICDE}}'06)},
  author = {Machanavajjhala, A. and Gehrke, J. and Kifer, D. and Venkitasubramaniam, M.},
  month = apr,
  year = {2006},
  keywords = {Computer science,Information analysis,Cardiac disease,Data privacy,Information resources,Insurance,Joining processes,Medical conditions,Medical diagnostic imaging,Publishing},
  pages = {24-24},
  file = {/Users/doisinkidney/Zotero/storage/3A784HFN/Machanavajjhala et al. - 2006 - L-diversity privacy beyond k-anonymity.pdf;/Users/doisinkidney/Zotero/storage/4AMFBX8C/1617392.html}
}

@inproceedings{pei_maintaining_2007,
  title = {Maintaining {{K}}-{{Anonymity}} against {{Incremental Updates}}},
  doi = {10.1109/SSDBM.2007.16},
  abstract = {K-anonymity is a simple yet practical mechanismto protect privacy against attacks of re-identifying individuals by joining multiple public data sources. All existing methods achieving k-anonymity assume implicitly that the data objects to be anonymized are given once and fixed. However, in many applications, the real world data sources are dynamic. In this paper, we investigate the problem of maintaining k-anonymity against incremental updates, and propose a simple yet effective solution. We analyze how inferences from multiple releases may temper the k-anonymity of data, and propose the monotonic incremental anonymization property. The general idea is to progressively and consistently reduce the generalization granularity as incremental updates arrive. Our new approach guarantees the k-anonymity on each release, and also on the inferred table using multiple releases. At the same time, our new approach utilizes the more and more accumulated data to reduce the information loss.},
  booktitle = {19th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}} ({{SSDBM}} 2007)},
  author = {Pei, J. and Xu, J. and Wang, Z. and Wang, W. and Wang, K.},
  month = jul,
  year = {2007},
  keywords = {data privacy,Data privacy,Conference management,database management system,database management systems,Databases,Diseases,k-anonymity,monotonic incremental anonymization property,Protection,public data source,real world data source},
  pages = {5-5},
  file = {/Users/doisinkidney/Zotero/storage/S7ZXMH72/Pei et al. - 2007 - Maintaining K-Anonymity against Incremental Update.pdf;/Users/doisinkidney/Zotero/storage/LM2G6IGV/4274950.html}
}

@inproceedings{lefevre_workload-aware_2006,
  address = {New York, NY, USA},
  series = {KDD '06},
  title = {Workload-Aware {{Anonymization}}},
  isbn = {978-1-59593-339-3},
  doi = {10.1145/1150402.1150435},
  abstract = {Protecting data privacy is an important problem in microdata distribution. Anonymization algorithms typically aim to protect individual privacy, with minimal impact on the quality of the resulting data. While the bulk of previous work has measured quality through one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used.This paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads, consisting of one or more data mining tasks, as well as selection predicates. An extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques.},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
  year = {2006},
  keywords = {privacy,anonymity,data recoding,predictive modeling},
  pages = {277--286},
  file = {/Users/doisinkidney/Zotero/storage/UG4AX7NU/LeFevre et al. - 2006 - Workload-aware Anonymization.pdf}
}

@inproceedings{xu_utility-based_2006,
  address = {New York, NY, USA},
  series = {KDD '06},
  title = {Utility-Based {{Anonymization Using Local Recoding}}},
  isbn = {978-1-59593-339-3},
  doi = {10.1145/1150402.1150504},
  abstract = {Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods.In this paper, we study the problem of utility-based anonymization. First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Xu, Jian and Wang, Wei and Pei, Jian and Wang, Xiaoyuan and Shi, Baile and Fu, Ada Wai-Chee},
  year = {2006},
  keywords = {data mining,local recoding,privacy preservation,k-anonymity,utility},
  pages = {785--790},
  file = {/Users/doisinkidney/Zotero/storage/6MW7AB7W/Xu et al. - 2006 - Utility-based Anonymization Using Local Recoding.pdf}
}

@inproceedings{iwuchukwu_k-anonymization_2007,
  address = {Vienna, Austria},
  series = {VLDB '07},
  title = {K-Anonymization {{As Spatial Indexing}}: {{Toward Scalable}} and {{Incremental Anonymization}}},
  isbn = {978-1-59593-649-3},
  shorttitle = {K-Anonymization {{As Spatial Indexing}}},
  abstract = {In this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. We use this observation to leverage over 20 years of work on database indexing to provide efficient and dynamic anonymization techniques. Experiments with our implementation show that the R-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. Finally, we show that the anonymizations generated by the R-tree approach do not sacrifice quality in their search for efficiency; in fact, by several previously proposed quality metrics, the compact partitioning properties of R-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms.},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Very Large Data Bases}}},
  publisher = {{VLDB Endowment}},
  author = {Iwuchukwu, Tochukwu and Naughton, Jeffrey F.},
  year = {2007},
  pages = {746--757},
  file = {/Users/doisinkidney/Zotero/storage/GQ4YCEL6/Iwuchukwu and Naughton - 2007 - K-anonymization As Spatial Indexing Toward Scalab.pdf}
}

@inproceedings{ghinita_fast_2007,
  address = {Vienna, Austria},
  series = {VLDB '07},
  title = {Fast {{Data Anonymization}} with {{Low Information Loss}}},
  isbn = {978-1-59593-649-3},
  abstract = {Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) The information loss metrics are counter-intuitive and fail to capture data inaccuracies inflicted for the sake of privacy. (ii) l-diversity is solved by techniques developed for the simpler k-anonymity problem, which introduces unnecessary inaccuracies. (iii) The anonymization process is inefficient in terms of computation and I/O cost. In this paper we propose a framework for efficient privacy preservation that addresses these deficiencies. First, we focus on one-dimensional (i.e., single attribute) quasi-identifiers, and study the properties of optimal solutions for k-anonymity and l-diversity, based on meaningful information loss metrics. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multi-dimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the state-of-the-art, in terms of execution time and information loss.},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Very Large Data Bases}}},
  publisher = {{VLDB Endowment}},
  author = {Ghinita, Gabriel and Karras, Panagiotis and Kalnis, Panos and Mamoulis, Nikos},
  year = {2007},
  pages = {758--769},
  file = {/Users/doisinkidney/Zotero/storage/SZY658H6/Ghinita et al. - 2007 - Fast Data Anonymization with Low Information Loss.pdf}
}

@inproceedings{wang_anonymizing_2006,
  address = {New York, NY, USA},
  series = {KDD '06},
  title = {Anonymizing {{Sequential Releases}}},
  isbn = {978-1-59593-339-3},
  doi = {10.1145/1150402.1150449},
  abstract = {An organization makes a new release as new information become available, releases a tailored view for each data request, releases sensitive information and identifying information separately. The availability of related releases sharpens the identification of individuals by a global quasi-identifier consisting of attributes from related releases. Since it is not an option to anonymize previously released data, the current release must be anonymized to ensure that a global quasi-identifier is not effective for identification. In this paper, we study the sequential anonymization problem under this assumption. A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose. We introduce the lossy join, a negative property in relational database design, as a way to hide the join relationship among releases, and propose a scalable and practical solution.},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Wang, Ke and Fung, Benjamin C. M.},
  year = {2006},
  keywords = {privacy,k-anonymity,classification,generalization,sequential release},
  pages = {414--423},
  file = {/Users/doisinkidney/Zotero/storage/E4XZVCEA/Wang and Fung - 2006 - Anonymizing Sequential Releases.pdf}
}

@inproceedings{iyengar_transforming_2002,
  address = {New York, NY, USA},
  series = {KDD '02},
  title = {Transforming {{Data}} to {{Satisfy Privacy Constraints}}},
  isbn = {978-1-58113-567-1},
  doi = {10.1145/775047.775089},
  abstract = {Data on individuals and entities are being collected widely. These data can contain information that explicitly identifies the individual (e.g., social security number). Data can also contain other kinds of personal information (e.g., date of birth, zip code, gender) that are potentially identifying when linked with other available data sets. Data are often shared for business or legal reasons. This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process. We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data. We extend earlier works in this area along various dimensions. First, satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated. This allows us to optimize the process of preserving privacy for the specified usage. In particular, we investigate the privacy transformation in the context of data mining applications like building classification and regression models. Second, our work improves on previous approaches by allowing more flexible generalizations for the data. Lastly, this is combined with a more thorough exploration of the solution space using the genetic algorithm framework. These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints.},
  booktitle = {Proceedings of the {{Eighth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Iyengar, Vijay S.},
  year = {2002},
  keywords = {privacy,predictive modeling,generalization,data transformation,suppression},
  pages = {279--288},
  file = {/Users/doisinkidney/Zotero/storage/XKGYEHHR/Iyengar - 2002 - Transforming Data to Satisfy Privacy Constraints.pdf}
}

@article{sugar_finding_2003,
  title = {Finding the {{Number}} of {{Clusters}} in a {{Dataset}}},
  volume = {98},
  issn = {0162-1459},
  doi = {10.1198/016214503000000666},
  abstract = {One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.},
  number = {463},
  journal = {Journal of the American Statistical Association},
  author = {Sugar, Catherine A. and James, Gareth M.},
  month = sep,
  year = {2003},
  keywords = {Information theory,Cluster analysis,Distortion,K-Means clustering,Mixture models},
  pages = {750-763},
  file = {/Users/doisinkidney/Zotero/storage/2XUG8WXZ/Sugar and James - 2003 - Finding the Number of Clusters in a Dataset.pdf;/Users/doisinkidney/Zotero/storage/FAKZUTDZ/016214503000000666.html}
}

@incollection{rokach_clustering_2005,
  title = {Clustering {{Methods}}},
  isbn = {978-0-387-24435-8 978-0-387-25465-4},
  abstract = {This chapter presents a tutorial overview of the main clustering methods used in Data Mining. The goal is to provide a self-contained review of the concepts and the mathematics underlying clustering techniques. The chapter begins by providing measures and criteria that are used for determining whether two objects are similar or dissimilar. Then the clustering methods are presented, divided into: hierarchical, partitioning, density-based, model-based, grid-based, and soft-computing methods. Following the methods, the challenges of performing clustering in large data sets are discussed. Finally, the chapter presents how to determine the number of clusters.},
  language = {en},
  booktitle = {Data {{Mining}} and {{Knowledge Discovery Handbook}}},
  publisher = {{Springer, Boston, MA}},
  author = {Rokach, Lior and Maimon, Oded},
  year = {2005},
  pages = {321-352},
  file = {/Users/doisinkidney/Zotero/storage/9AE2KLJF/Rokach and Maimon - 2005 - Clustering Methods.pdf;/Users/doisinkidney/Zotero/storage/8ZENXL94/0-387-25465-X_15.html},
  doi = {10.1007/0-387-25465-X_15}
}

@article{hadi_finding_1992,
  title = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  volume = {34},
  issn = {00401706},
  shorttitle = {Finding {{Groups}} in {{Data}}},
  doi = {10.2307/1269576},
  language = {en},
  number = {1},
  journal = {Technometrics},
  author = {Hadi, Ali S. and Kaufman, L. and Rousseeuw, P. J.},
  month = feb,
  year = {1992},
  pages = {111},
  file = {/Users/doisinkidney/Zotero/storage/L8QC8H6E/9780470316801.pdf}
}

@article{roux_comparative_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.08977},
  primaryClass = {cs, q-bio},
  title = {A Comparative Study of Divisive Hierarchical Clustering Algorithms},
  abstract = {A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps : first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the nodes levels of the resulting dendrogram. A number of such algorithms is given. These algorithms are compared using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these comparisons are in favor of two methods based on unusual ratio-type formulas for the splitting procedures, namely the Silhouette criterion and Dunn's criterion. These two criteria take into account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method.},
  journal = {arXiv:1506.08977 [cs, q-bio]},
  author = {Roux, Maurice},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,62-07,Quantitative Biology - Quantitative Methods},
  file = {/Users/doisinkidney/Zotero/storage/ALHLXHH4/Roux - 2015 - A comparative study of divisive hierarchical clust.pdf},
  annote = {Comment: 11 pages, 1 figure}
}

@article{xu_comprehensive_2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  volume = {2},
  issn = {2198-5804, 2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
  language = {en},
  number = {2},
  journal = {Annals of Data Science},
  author = {Xu, Dongkuan and Tian, Yingjie},
  month = jun,
  year = {2015},
  pages = {165-193},
  file = {/Users/doisinkidney/Zotero/storage/Y6VP7RY2/Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf}
}

@article{frey_clustering_2007,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  volume = {315},
  copyright = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1136800},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such ``exemplars'' can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called ``affinity propagation,'' which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.
An algorithm that exchanges messages about the similarity of pairs of data points speeds identification of representative examples in a complex data set, such as genes in DNA data.
An algorithm that exchanges messages about the similarity of pairs of data points speeds identification of representative examples in a complex data set, such as genes in DNA data.},
  language = {en},
  number = {5814},
  journal = {Science},
  author = {Frey, Brendan J. and Dueck, Delbert},
  month = feb,
  year = {2007},
  pages = {972-976},
  file = {/Users/doisinkidney/Zotero/storage/3PEYS54S/Frey and Dueck - 2007 - Clustering by Passing Messages Between Data Points.pdf;/Users/doisinkidney/Zotero/storage/CNSZRGFN/972.html},
  pmid = {17218491}
}

@incollection{kotagiri_efficient_2007,
  address = {Berlin, Heidelberg},
  title = {Efficient K-{{Anonymization Using Clustering Techniques}}},
  volume = {4443},
  isbn = {978-3-540-71702-7 978-3-540-71703-4},
  language = {en},
  booktitle = {Advances in {{Databases}}: {{Concepts}}, {{Systems}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Byun, Ji-Won and Kamra, Ashish and Bertino, Elisa and Li, Ninghui},
  editor = {Kotagiri, Ramamohanarao and Krishna, P. Radha and Mohania, Mukesh and Nantajeewarawat, Ekawit},
  year = {2007},
  pages = {188-200},
  file = {/Users/doisinkidney/Zotero/storage/KD58W972/Byun et al. - 2007 - Efficient k-Anonymization Using Clustering Techniq.pdf},
  doi = {10.1007/978-3-540-71703-4_18}
}

@article{aggarwal_achieving_2010,
  title = {Achieving Anonymity via Clustering},
  volume = {6},
  issn = {15496325},
  doi = {10.1145/1798596.1798602},
  language = {en},
  number = {3},
  journal = {ACM Transactions on Algorithms},
  author = {Aggarwal, Gagan and Panigrahy, Rina and Feder, Tom{\'a}s and Thomas, Dilys and Kenthapadi, Krishnaram and Khuller, Samir and Zhu, An},
  month = jun,
  year = {2010},
  pages = {1-19},
  file = {/Users/doisinkidney/Zotero/storage/CC7JDSPL/Aggarwal et al. - 2010 - Achieving anonymity via clustering.pdf}
}

@article{nergiz_multirelational_2009,
  title = {Multirelational K-{{Anonymity}}},
  volume = {21},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2008.210},
  abstract = {k-anonymity protects privacy by ensuring that data cannot be linked to a single individual. In a k-anonymous data set, any identifying information occurs in at least k tuples. Much research has been done to modify a single-table data set to satisfy anonymity constraints. This paper extends the definitions of k-anonymity to multiple relations and shows that previously proposed methodologies either fail to protect privacy or overly reduce the utility of the data in a multiple relation setting. We also propose two new clustering algorithms to achieve multirelational anonymity. Experiments show the effectiveness of the approach in terms of utility and efficiency.},
  number = {8},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Nergiz, M. E. and Clifton, C. and Nergiz, A. E.},
  month = aug,
  year = {2009},
  keywords = {Relational databases,data privacy,Privacy,Security,security,privacy protection,and protection,clustering algorithms,integrity,multiple relation setting,multirelational k-anonymity,protection.,relational database,security of data},
  pages = {1104-1117},
  file = {/Users/doisinkidney/Zotero/storage/NCQ8N6B8/Nergiz et al. - 2009 - Multirelational k-Anonymity.pdf;/Users/doisinkidney/Zotero/storage/7HWYAXU4/4653492.html}
}

@incollection{zhu_achieving_2007,
  series = {Lecture Notes in Computer Science},
  title = {Achieving K-{{Anonymity Via}} a {{Density}}-{{Based Clustering Method}}},
  isbn = {978-3-540-72483-4 978-3-540-72524-4},
  abstract = {The key idea of our k-anonymity is to cluster the personal data based on the density which is measured by the k-Nearest-Neighbor (KNN) distance. We add a constraint that each cluster contains at least k records which is not the same as the traditional clustering methods, and provide an algorithm to come up with such a clustering. We also develop more appropriate metrics to measure the distance and information loss, which is suitable in both numeric and categorical attributes. Experiment results show that our algorithm causes significantly less information loss than previous proposed clustering algorithms.},
  language = {en},
  booktitle = {Advances in {{Data}} and {{Web Management}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Zhu, Hua and Ye, Xiaojun},
  year = {2007},
  pages = {745-752},
  file = {/Users/doisinkidney/Zotero/storage/IJFGH7RV/Zhu and Ye - 2007 - Achieving Emphasis Type=ItalickEmphasis-Ano.pdf;/Users/doisinkidney/Zotero/storage/S6ETR46V/978-3-540-72524-4_76.html},
  doi = {10.1007/978-3-540-72524-4_76}
}

@inproceedings{zhang_k-anonymity_2014,
  title = {A {{K}}-Anonymity Clustering Algorithm Based on the Information Entropy},
  doi = {10.1109/CSCWD.2014.6846862},
  abstract = {Data anonymization techniques are the main way to achieve privacy protection, and as a classical anonymity model, K-anonymity is the most effective and frequently-used. But the majority of K-anonymity algorithms can hardly balance the data quality and efficiency, and ignore the privacy of the data to improve the data quality. To solve the problems above, by introducing the concept of ``diameter'' and a new clustering criterion based on the parameter of the maximum threshold of equivalence classes, we proposed a K-anonymity clustering algorithm based on the information entropy. The results of experiments showed that both the algorithm efficiency and data security are improved, and meanwhile the total information loss is acceptable, so the proposed algorithm has some practicability in application.},
  booktitle = {Proceedings of the 2014 {{IEEE}} 18th {{International Conference}} on {{Computer Supported Cooperative Work}} in {{Design}} ({{CSCWD}})},
  author = {Zhang, J. and Zhao, Y. and Yang, Y. and Yang, J.},
  month = may,
  year = {2014},
  keywords = {data privacy,Clustering algorithms,Information entropy,privacy protection,Entropy,security of data,Algorithm design and analysis,classical anonymity model,Classification algorithms,clustering,data anonymization techniques,data efficiency,data quality improvement,data security,Data security,entropy,information entropy,K-anonymity,K-anonymity clustering algorithm,Loss measurement,maximum equivalence class threshold,pattern clustering,privacy preserving},
  pages = {319-324},
  file = {/Users/doisinkidney/Zotero/storage/BVHXFPTX/Zhang et al. - 2014 - A K-anonymity clustering algorithm based on the in.pdf;/Users/doisinkidney/Zotero/storage/CENGN84V/6846862.html}
}

@article{holohan_k-epsilon-anonymity_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.01615},
  primaryClass = {cs, math},
  title = {K-Epsilon-{{Anonymity}}: K-{{Anonymity}} with Epsilon-{{Differential Privacy}}},
  shorttitle = {(\$k\$,\$$\backslash$epsilon\$)-{{Anonymity}}},
  abstract = {The explosion in volume and variety of data offers enormous potential for research and commercial use. Increased availability of personal data is of particular interest in enabling highly customised services tuned to individual needs. Preserving the privacy of individuals against reidentification attacks in this fast-moving ecosystem poses significant challenges for a one-size fits all approach to anonymisation. In this paper we present (\$k\$,\$$\backslash$epsilon\$)-anonymisation, an approach that combines the \$k\$-anonymisation and \$$\backslash$epsilon\$-differential privacy models into a single coherent framework, providing privacy guarantees at least as strong as those offered by the individual models. Linking risks of less than 5$\backslash$\% are observed in experimental results, even with modest values of \$k\$ and \$$\backslash$epsilon\$. Our approach is shown to address well-known limitations of \$k\$-anonymity and \$$\backslash$epsilon\$-differential privacy and is validated in an extensive experimental campaign using openly available datasets.},
  journal = {arXiv:1710.01615 [cs, math]},
  author = {Holohan, Naoise and Antonatos, Spiros and Braghin, Stefano and Mac Aonghusa, P{\'o}l},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Databases,Mathematics - Probability},
  file = {/Users/doisinkidney/Zotero/storage/6IDLCUV3/Holohan et al. - 2017 - ($k$,$epsilon$)-Anonymity $k$-Anonymity with $e.pdf;/Users/doisinkidney/Zotero/storage/LC87D4FA/1710.html}
}

@inproceedings{ester_density-based_1996,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  publisher = {{AAAI Press}},
  author = {Ester, Martin and Kriegel, Hans-peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  pages = {226--231},
  file = {/Users/doisinkidney/Zotero/storage/BCVPTKZ8/Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf;/Users/doisinkidney/Zotero/storage/FKA3CXQ7/summary.html}
}

@incollection{hutchison_pattern-guided_2013,
  address = {Berlin, Heidelberg},
  title = {Pattern-{{Guided}} k-{{Anonymity}}},
  volume = {7924},
  isbn = {978-3-642-38755-5 978-3-642-38756-2},
  abstract = {We suggest a user-oriented approach to combinatorial data anonymization. A data matrix is called k-anonymous if every row appears at least k times\textemdash{}the goal of the NP-hard k-Anonymity problem then is to make a given matrix k-anonymous by suppressing (blanking out) as few entries as possible. We describe an enhanced k-anonymization problem called Pattern-Guided k-Anonymity where the users can express the differing importance of various data features. We show that PatternGuided k-Anonymity remains NP-hard. We provide a fixed-parameter tractability result based on a data-driven parameterization and, based on this, develop an exact ILP-based solution method as well as a simple but very effective greedy heuristic. Experiments on several real-world datasets show that our heuristic easily matches up to the established ``Mondrian'' algorithm for k-Anonymity in terms of quality of the anonymization and outperforms it in terms of running time.},
  language = {en},
  booktitle = {Frontiers in {{Algorithmics}} and {{Algorithmic Aspects}} in {{Information}} and {{Management}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bredereck, Robert and Nichterlein, Andr{\'e} and Niedermeier, Rolf},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fellows, Michael and Tan, Xuehou and Zhu, Binhai},
  year = {2013},
  pages = {350-361},
  file = {/Users/doisinkidney/Zotero/storage/RHCW6J6W/Bredereck et al. - 2013 - Pattern-Guided k-Anonymity.pdf},
  doi = {10.1007/978-3-642-38756-2_35}
}

@article{ayala-rivera_systematic_2014,
  title = {A {{Systematic Comparison}} and {{Evaluation}} of K-{{Anonymization Algorithms}} for {{Practitioners}}},
  abstract = {The vast amount of data being collected about individuals has brought new challenges in protecting their privacy when this data is disseminated. As a result, Privacy-Preserving Data Publishing has become an active research area, in which multiple anonymization algorithms have been proposed. However, given the large number of algorithms available and limited information regarding their performance, it is difficult to identify and select the most appropriate algorithm given a particular publishing scenario, especially for practitioners. In this paper, we perform a systematic comparison of three well-known k-anonymization algorithms to measure their efficiency (in terms of resources usage) and their effectiveness (in terms of data utility). We extend the scope of their original evaluation by employing a more comprehensive set of scenarios: different parameters, metrics and datasets. Using publicly available implementations of those algorithms, we conduct a series of experiments and a comprehensive analysis to identify the factors that influence their performance, in order to guide practitioners in the selection of an algorithm. We demonstrate through experimental evaluation, the conditions in which one algorithm outperforms the others for a particular metric, depending on the input dataset and privacy requirements. Our findings motivate the necessity of creating methodologies that provide recommendations about the best algorithm given a particular publishing scenario.},
  language = {en},
  author = {Ayala-Rivera, Vanessa and McDonagh, Patrick and Cerqueus, Thomas and Murphy, Liam},
  year = {2014},
  pages = {34},
  file = {/Users/doisinkidney/Zotero/storage/IMVTTGD3/Ayala-Rivera et al. - 2014 - A Systematic Comparison and Evaluation of k-Anonym.pdf}
}

@article{nievergelt_binary_1973,
  title = {Binary {{Search Trees}} of {{Bounded Balance}}},
  volume = {2},
  issn = {0097-5397},
  doi = {10.1137/0202005},
  abstract = {A new class of binary search trees, called trees of bounded balance, is introduced. These trees are easy to maintain in their form despite insertions and deletions of nodes, and the search time is only moderately longer than in completely balanced trees. Trees of bounded balance differ from other classes of binary search trees in that they contain a parameter which can be varied so the compromise between short search time and infrequent restructuring can be chosen arbitrarily.},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Nievergelt, J. and Reingold, E.},
  month = mar,
  year = {1973},
  pages = {33-43},
  file = {/Users/doisinkidney/Zotero/storage/MPZR6HG4/Nievergelt and Reingold - 1973 - Binary Search Trees of Bounded Balance.pdf;/Users/doisinkidney/Zotero/storage/L64PKNMB/0202005.html}
}

@article{hu_optimal_1971,
  title = {Optimal {{Computer Search Trees}} and {{Variable}}-{{Length Alphabetical Codes}}},
  volume = {21},
  issn = {0036-1399},
  doi = {10.1137/0121057},
  abstract = {An algorithm is given for constructing an alphabetic binary tree of minimum weighted path length (for short, an optimal alphabetic tree). The algorithm needs \$4n\^2  + 2n\$ operations and \$4n\$ storage locations, where n is the number of terminal nodes in the tree. A given binary tree corresponds to a computer search procedure, where the given files or letters (represented by terminal nodes) are partitioned into two parts successively until a particular file or letter is finally identified. If the files or letters are listed alphabetically, such as a dictionary, then the binary tree must have, from left to right, the terminal nodes consecutively. Since different letters have different frequencies (weights) of occurring,  an alphabetic tree of minimum weighted path length corresponds to a computer search tree with minimum-mean search time. A binary tree also represents a (variable-length) binary code. In an alphabetic binary code, the numerical binary order of the code words corresponds to the alphabetical order of the encoded letters. An optimal alphabetic tree corresponds to an optimal alphabetic binary code.},
  number = {4},
  journal = {SIAM Journal on Applied Mathematics},
  author = {Hu, Te C and Tucker, Alan},
  month = dec,
  year = {1971},
  pages = {514-532},
  file = {/Users/doisinkidney/Zotero/storage/CCPC7Z9I/Hu and Tucker - 1971 - Optimal Computer Search Trees and Variable-Length .pdf;/Users/doisinkidney/Zotero/storage/PAGAW74V/0121057.html}
}

@article{podgursky_practical_-1,
  title = {{{PRACTICAL K}}-{{ANONYMITY ON LARGE DATASETS}}},
  language = {en},
  author = {Podgursky, Benjamin},
  pages = {64},
  file = {/Users/doisinkidney/Zotero/storage/8I7ZB9DC/Podgursky - PRACTICAL K-ANONYMITY ON LARGE DATASETS.pdf;/Users/doisinkidney/Zotero/storage/9XTYQVNL/Podgursky - PRACTICAL K-ANONYMITY ON LARGE DATASETS.pdf}
}

@inproceedings{gionis_k-anonymization_2008,
  title = {K-{{Anonymization Revisited}}},
  doi = {10.1109/ICDE.2008.4497483},
  abstract = {In this paper we introduce new notions of k-type anonymizations. Those notions achieve similar privacy goals as those aimed by Sweenie and Samarati when proposing the concept of k-anonymization: an adversary who knows the public data of an individual cannot link that individual to less than k records in the anonymized table. Every anonymized table that satisfies k-anonymity complies also with the anonymity constraints dictated by the new notions, but the converse is not necessarily true. Thus, those new notions allow generalized tables that may offer higher utility than k-anonymized tables, while still preserving the required privacy constraints. We discuss and compare the new anonymization concepts, which we call (1,k)-, (k, k)- and global (1, k)-anonymizations, according to several utility measures. We propose a collection of agglomerative algorithms for the problem of finding such anonymizations with high utility, and demonstrate the usefulness of our definitions and our algorithms through extensive experimental evaluation on real and synthetic datasets.},
  booktitle = {2008 {{IEEE}} 24th {{International Conference}} on {{Data Engineering}}},
  author = {Gionis, A. and Mazza, A. and Tassa, T.},
  month = apr,
  year = {2008},
  keywords = {Computer science,data privacy,k-anonymization,Data privacy,Databases,Protection,Data security,Information security,agglomerative algorithm,Cost function,Data mining,Hospitals,k-anonymized table,k-type anonymization,privacy goals},
  pages = {744-753},
  file = {/Users/doisinkidney/Zotero/storage/2QPAV3XE/Gionis et al. - 2008 - k-Anonymization Revisited.pdf;/Users/doisinkidney/Zotero/storage/RL7KMJK9/4497483.html}
}

@article{nergiz_thoughts_,
  title = {Thoughts on K-{{Anonymization}} ${_\ast}$},
  language = {en},
  author = {Nergiz, M Ercan and Clifton, Chris},
  pages = {10},
  file = {/Users/doisinkidney/Zotero/storage/WYSDZ2RC/Nergiz and Clifton - Thoughts on k-Anonymization ∗.pdf}
}

@article{_limiting_2012,
  title = {Limiting Disclosure of Sensitive Data in Sequential Releases of Databases},
  volume = {191},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2011.12.020},
  abstract = {Privacy Preserving Data Publishing (PPDP) is a research field that deals with the development of methods to enable publishing of data while minimizing\ldots{}},
  language = {en},
  journal = {Information Sciences},
  month = may,
  year = {2012},
  pages = {98-127},
  file = {/Users/doisinkidney/Zotero/storage/5H9CIG23/S0020025511006694.html}
}

@inproceedings{xue_anonymizing_2012,
  address = {New York, NY, USA},
  series = {KDD '12},
  title = {Anonymizing {{Set}}-Valued {{Data}} by {{Nonreciprocal Recoding}}},
  isbn = {978-1-4503-1462-6},
  doi = {10.1145/2339530.2339696},
  abstract = {Today there is a strong interest in publishing set-valued data in a privacy-preserving manner. Such data associate individuals to sets of values (e.g., preferences, shopping items, symptoms, query logs). In addition, an individual can be associated with a sensitive label (e.g., marital status, religious or political conviction). Anonymizing such data implies ensuring that an adversary should not be able to (1) identify an individual's record, and (2) infer a sensitive label, if such exists. Existing research on this problem either perturbs the data, publishes them in disjoint groups disassociated from their sensitive labels, or generalizes their values by assuming the availability of a generalization hierarchy. In this paper, we propose a novel alternative. Our publication method also puts data in a generalized form, but does not require that published records form disjoint groups and does not assume a hierarchy either; instead, it employs generalized bitmaps and recasts data values in a nonreciprocal manner; formally, the bipartite graph from original to anonymized records does not have to be composed of disjoint complete subgraphs. We configure our schemes to provide popular privacy guarantees while resisting attacks proposed in recent research, and demonstrate experimentally that we gain a clear utility advantage over the previous state of the art.},
  booktitle = {Proceedings of the 18th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Xue, Mingqiang and Karras, Panagiotis and Ra{\"\i}ssi, Chedy and Vaidya, Jaideep and Tan, Kian-Lee},
  year = {2012},
  keywords = {privacy,anonymization,nonreciprocal recoding,set-valued data},
  pages = {1050--1058},
  file = {/Users/doisinkidney/Zotero/storage/JYKWJU65/Xue et al. - 2012 - Anonymizing Set-valued Data by Nonreciprocal Recod.pdf}
}

@inproceedings{wong_non-homogeneous_2010,
  title = {Non-Homogeneous Generalization in Privacy Preserving Data Publishing},
  isbn = {978-1-4503-0032-2},
  doi = {10.1145/1807167.1807248},
  abstract = {Most previous research on privacy-preserving data publishing, based on the k-anonymity model, has followed the simplistic approach of homogeneously giving the same generalized value in all quasi-identifiers within a partition. We observe that the anonymization error can be reduced if we follow a non-homogeneous generalization approach for groups of size larger than k. Such an approach would allow tuples within a partition to take different generalized quasi-identifier values. Anonymization following this model is not trivial, as its direct application can easily violate kanonymity. In addition, non-homogeneous generalization allows for additional types of attack, which should be considered in the process. We provide a methodology for verifying whether a nonhomogeneous generalization violates k-anonymity. Then, we propose a technique that generates a non-homogeneous generalization for a partition and show that its result satisfies k-anonymity, however by straightforwardly applying it, privacy can be compromised if the attacker knows the anonymization algorithm. Based on this, we propose a randomization method that prevents this type of attack and show that k-anonymity is not compromised by it. Nonhomogeneous generalization can be used on top of any existing partitioning approach to improve its utility. In addition, we show that a new partitioning technique tailored for non-homogeneous generalization can further improve quality. A thorough experimental evaluation demonstrates that our methodology greatly improves the utility of anonymized data in practice.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Wong, Wai Kit and Mamoulis, Nikos and Cheung, David Wai Lok},
  year = {2010},
  pages = {747},
  file = {/Users/doisinkidney/Zotero/storage/3JNFNXGL/Wong et al. - 2010 - Non-homogeneous generalization in privacy preservi.pdf}
}

@inproceedings{kifer_attacks_2009,
  title = {Attacks on Privacy and {{deFinetti}}'s Theorem},
  isbn = {978-1-60558-551-2},
  doi = {10.1145/1559845.1559861},
  abstract = {In this paper we present a method for reasoning about privacy using the concepts of exchangeability and deFinetti's theorem. We illustrate the usefulness of this technique by using it to attack a popular data sanitization scheme known as Anatomy. We stress that Anatomy is not the only sanitization scheme that is vulnerable to this attack. In fact, any scheme that uses the random worlds model, i.i.d. model, or tuple-independent model needs to be re-evaluated.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Kifer, Daniel},
  year = {2009},
  pages = {127},
  file = {/Users/doisinkidney/Zotero/storage/5SZALRY3/Kifer - 2009 - Attacks on privacy and deFinetti's theorem.pdf}
}

@book{a_limiting_,
  title = {Limiting {{Disclosure}} of {{Sensitive Data}} in {{Sequential Releases}} of {{Databases}}},
  abstract = {E. Shmueli and T. Tassa contributed equally to this work. Privacy Preserving Data Publishing (PPDP) is a research field that deals with the development of methods to enable publishing of data while minimizing distortion, for maintaining usability on one hand, and respecting privacy on the other hand. Sequential release is a scenario of data publishing where multiple releases of the same underlying table are published over a period of time. A violation of privacy, in this case, may emerge from any one of the releases, or as a result of joining information from different releases. Similarly to [37], our privacy definitions limit the ability of an adversary who combines information from all releases, to link values of the quasi-identifiers to sensitive values. We extend the framework that was considered in [37] in three ways: We allow a greater number of releases, we consider the more flexible local recoding model of ``cell generalization '' (as opposed to the global recoding model of ``cut generalization '' in [37]), and we include the case where records may be added to the underlying table from time to time. Our extension of the framework requires also to modify the manner in which privacy is evaluated. We show},
  author = {A, Erez Shmueli and B, Tamir Tassa and A, Raz Wasserstein and A, Bracha Shapira and A, Lior Rokach},
  file = {/Users/doisinkidney/Zotero/storage/KLZBS93H/A et al. - Limiting Disclosure of Sensitive Data in Sequentia.pdf;/Users/doisinkidney/Zotero/storage/RJNHWIXI/summary.html}
}

@article{bajaj_practical_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.06508},
  primaryClass = {cs},
  title = {Practical {{Foundations}} of {{History Independence}}},
  abstract = {The way data structures organize data is often a function of the sequence of past operations. The organization of data is referred to as the data structure's state, and the sequence of past operations constitutes the data structure's history. A data structure state can therefore be used as an oracle to derive information about its history. As a result, for history-sensitive applications, such as privacy in e-voting, incremental signature schemes, and regulatory compliant data retention; it is imperative to conceal historical information contained within data structure states. Data structure history can be hidden by making data structures history independent. In this paper, we explore how to achieve history independence. We observe that current history independence notions are significantly limited in number and scope. There are two existing notions of history independence -- weak history independence (WHI) and strong history independence (SHI). WHI does not protect against insider adversaries and SHI mandates canonical representations, resulting in inefficiency. We postulate the need for a broad, encompassing notion of history independence, which can capture WHI, SHI, and a broad spectrum of new history independence notions. To this end, we introduce \$$\backslash$Delta\$history independence (\$$\backslash$Delta\$HI), a generic game-based framework that is malleable enough to accommodate existing and new history independence notions. As an essential step towards formalizing \$$\backslash$Delta\$HI, we explore the concepts of abstract data types, data structures, machine models, memory representations and history independence. Finally, to bridge the gap between theory and practice, we outline a general recipe for building end-to-end, history independent systems and demonstrate the use of the recipe in designing two history independent file systems.},
  journal = {arXiv:1501.06508 [cs]},
  author = {Bajaj, Sumeet and Chakraborti, Anrin and Sion, Radu},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/doisinkidney/Zotero/storage/VTWXDRCQ/Bajaj et al. - 2015 - Practical Foundations of History Independence.pdf;/Users/doisinkidney/Zotero/storage/FT6ICPR3/1501.html}
}

@article{sweeney_k-anonymity_2002-1,
  title = {K-Anonymity: {{A Model}} for {{Protecting Privacy}}},
  volume = {10},
  issn = {0218-4885},
  shorttitle = {K-Anonymity},
  doi = {10.1142/S0218488502001648},
  abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to k- anonymity unless accompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world systems known as Datafly, $\mathrm{\mu}$-Argus and k-Similar provide guarantees of privacy protection.},
  number = {5},
  journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
  author = {Sweeney, Latanya},
  month = oct,
  year = {2002},
  keywords = {data privacy,privacy,data anonymity,data fusion,re-identification},
  pages = {557--570},
  file = {/Users/doisinkidney/Zotero/storage/FIEUNLJI/Sweeney - 2002 - k-ANONYMITY A MODEL FOR PROTECTING PRIVACY 1.pdf;/Users/doisinkidney/Zotero/storage/99BNRQKY/summary.html}
}

@inproceedings{li_t-closeness_2007,
  title = {T-{{Closeness}}: {{Privacy Beyond}} k-{{Anonymity}} and l-{{Diversity}}},
  shorttitle = {T-{{Closeness}}},
  doi = {10.1109/ICDE.2007.367856},
  abstract = {The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain "identifying" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.},
  booktitle = {2007 {{IEEE}} 23rd {{International Conference}} on {{Data Engineering}}},
  author = {Li, N. and Li, T. and Venkatasubramanian, S.},
  month = apr,
  year = {2007},
  keywords = {Computer science,data privacy,Privacy,Publishing,Databases,Diseases,Protection,Data security,attribute disclosure,database theory,Earth,earth mover distance measure,equivalence class,identifying attributes,k-anonymity privacy requirement,l-diversity,microdata publishing,Motion measurement,Remuneration,t-closeness},
  pages = {106-115},
  file = {/Users/doisinkidney/Zotero/storage/HJN44YDS/Li et al. - 2007 - t-Closeness Privacy Beyond k-Anonymity and l-Dive.pdf;/Users/doisinkidney/Zotero/storage/PWK47BRU/4221659.html}
}

@article{shi_normalized_2000,
  title = {Normalized {{Cuts}} and {{Image Segmentation}}},
  volume = {22},
  abstract = {{\DH}We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
  language = {en},
  number = {8},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  author = {Shi, Jianbo and Malik, Jitendra},
  year = {2000},
  pages = {18},
  file = {/Users/doisinkidney/Zotero/storage/375JL3X4/Shi and Malik - 2000 - Normalized Cuts and Image Segmentation.pdf}
}

@article{brown_building_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.5420},
  primaryClass = {cs},
  title = {Building a {{Balanced}} K-d {{Tree}} in {{O}}(Kn Log n) {{Time}}},
  abstract = {The original description of the k-d tree recognized that rebalancing techniques, such as are used to build an AVL tree or a red-black tree, are not applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is necessary to find the median of the data for each recursive subdivision of those data. The sort or selection that is used to find the median for each subdivision strongly influences the computational complexity of building a k-d tree. This paper discusses an alternative algorithm that builds a balanced k-d tree by presorting the data in each of k dimensions prior to building the tree. It then preserves the order of these k sorts during tree construction and thereby avoids the requirement for any further sorting. Moreover, this algorithm is amenable to parallel execution via multiple threads. Compared to an algorithm that finds the median for each recursive subdivision, this presorting algorithm has equivalent performance for four dimensions and better performance for three or fewer dimensions.},
  journal = {arXiv:1410.5420 [cs]},
  author = {Brown, Russell A.},
  month = oct,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/doisinkidney/Zotero/storage/8DUXYIUX/Brown - 2014 - Building a Balanced k-d Tree in O(kn log n) Time.pdf;/Users/doisinkidney/Zotero/storage/LYJ89Z5V/1410.html},
  annote = {Comment: 8 pages, 7 figures, published at http://jcgt.org/published/0004/01/03/}
}

@inproceedings{arya_algorithms_1993,
  title = {Algorithms for Fast Vector Quantization},
  doi = {10.1109/DCC.1993.253111},
  abstract = {This paper shows that if one is willing to relax the requirement of finding the true nearest neighbor, it is possible to achieve significant improvements in running time and at only a very small loss in the performance of the vector quantizer. The authors present three algorithms for nearest neighbor searching: standard and priority k -d tree search algorithms and a neighborhood graph search algorithm in which a directed graph is constructed for the point set and edges join neighboring points},
  booktitle = {[{{Proceedings}}] {{DCC}} `93: {{Data Compression Conference}}},
  author = {Arya, S. and Mount, D. M.},
  year = {1993},
  keywords = {Computer science,performance,Tree graphs,search problems,Image coding,Application software,Educational institutions,Books,directed graph,directed graphs,fast vector quantization,Nearest neighbor searches,neighborhood graph search algorithm,Performance loss,running time,Speech,tree search algorithms,vector quantisation,Vector quantization},
  pages = {381-390},
  file = {/Users/doisinkidney/Zotero/storage/RRTMGVRH/253111.html}
}

@inproceedings{scott_ebs_2003,
  series = {Lecture Notes in Computer Science},
  title = {{{EBS}} K-d {{Tree}}: {{An Entropy Balanced Statistical}} k-d {{Tree}} for {{Image Databases}} with {{Ground}}-{{Truth Labels}}},
  isbn = {978-3-540-40634-1 978-3-540-45113-6},
  shorttitle = {{{EBS}} K-d {{Tree}}},
  doi = {10.1007/3-540-45113-7_46},
  abstract = {In this paper we present a new image database indexing structure \textemdash{} Entropy Balanced Statistical (EBS) k-d Tree. This indexing mechanism utilizes the statistical properties and ground-truth labeling of image data for efficient and accurate searches. It is particularly valuable in the domains of medical and biological image database retrieval, where ground-truth labeling are available and archived with the images. The EBS k-d tree is an extension to the statistical k-d tree that attempts to optimize a multi-dimensional decision tree based on the fundamental principles from which it is constructed. Our approach is to develop and validate the notion of an entropy balanced statistical based decision tree. It is shown that by making balanced split decisions in the growth processing of the tree, that the average search depth is improved and the worst case search depth is usually dramatically improved. Furthermore, a method for linking the tree leaves into a non-linear structure was developed to increase the n-nearest neighbor similarity search accuracy. We have applied this to a large-scale medical diagnostic image database and have shown increases in search speed and accuracy over an ordinary distance-based search and the original statistical k-d tree index.},
  language = {en},
  booktitle = {Image and {{Video Retrieval}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Scott, Grant J. and Shyu, Chi-Ren},
  month = jul,
  year = {2003},
  pages = {467-477},
  file = {/Users/doisinkidney/Zotero/storage/QXRWWLSY/Scott and Shyu - 2003 - EBS k-d Tree An Entropy Balanced Statistical k-d .pdf;/Users/doisinkidney/Zotero/storage/TBR8K5IS/3-540-45113-7_46.html}
}

@inproceedings{arya_algorithms_1993-2,
  title = {Algorithms for Fast Vector Quantization},
  doi = {10.1109/DCC.1993.253111},
  abstract = {This paper shows that if one is willing to relax the requirement of finding the true nearest neighbor, it is possible to achieve significant improvements in running time and at only a very small loss in the performance of the vector quantizer. The authors present three algorithms for nearest neighbor searching: standard and priority k -d tree search algorithms and a neighborhood graph search algorithm in which a directed graph is constructed for the point set and edges join neighboring points},
  booktitle = {[{{Proceedings}}] {{DCC}} `93: {{Data Compression Conference}}},
  author = {Arya, S. and Mount, D. M.},
  year = {1993},
  keywords = {Computer science,performance,Tree graphs,search problems,Image coding,Application software,Educational institutions,Books,directed graph,directed graphs,fast vector quantization,Nearest neighbor searches,neighborhood graph search algorithm,Performance loss,running time,Speech,tree search algorithms,vector quantisation,Vector quantization},
  pages = {381-390},
  file = {/Users/doisinkidney/Zotero/storage/472Y935X/Arya and Mount - 1993 - Algorithms for fast vector quantization.pdf;/Users/doisinkidney/Zotero/storage/M7QJJ45P/253111.html}
}

@article{hall_random_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1112.2680},
  primaryClass = {cs, stat},
  title = {Random {{Differential Privacy}}},
  abstract = {We propose a relaxed privacy definition called \{$\backslash$em random differential privacy\} (RDP). Differential privacy requires that adding any new observation to a database will have small effect on the output of the data-release procedure. Random differential privacy requires that adding a \{$\backslash$em randomly drawn new observation\} to a database will have small effect on the output. We show an analog of the composition property of differentially private procedures which applies to our new definition. We show how to release an RDP histogram and we show that RDP histograms are much more accurate than histograms obtained using ordinary differential privacy. We finally show an analog of the global sensitivity framework for the release of functions under our privacy definition.},
  journal = {arXiv:1112.2680 [cs, stat]},
  author = {Hall, Rob and Rinaldo, Alessandro and Wasserman, Larry},
  month = dec,
  year = {2011},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Learning,Statistics - Methodology},
  file = {/Users/doisinkidney/Zotero/storage/847QG4BF/Hall et al. - 2011 - Random Differential Privacy.pdf;/Users/doisinkidney/Zotero/storage/TFLYF3NG/1112.html}
}

@article{garsia_new_1977,
  title = {A {{New Algorithm}} for {{Minimum Cost Binary Trees}}},
  volume = {6},
  issn = {0097-5397},
  doi = {10.1137/0206045},
  abstract = {A new algorithm for constructing minimum cost binary trees in \$O(n $\backslash$log n)\$ time is presented. The algorithm is similar to the well-known Hu-Tucker algorithm. Our proof of validity is based on finite variational methods and is therefore quite different and somewhat simpler than the proof for the Hu-Tucker algorithm. Our proof also yields some additional information about the structure of minimum cost binary trees. This permits a linear time implementation of our algorithm in a special case.},
  number = {4},
  journal = {SIAM Journal on Computing},
  author = {Garsia, A. and Wachs, M.},
  month = dec,
  year = {1977},
  pages = {622-642},
  file = {/Users/doisinkidney/Zotero/storage/WH2HVVMB/0206045.html}
}

@article{hu_binary_1979,
  title = {Binary {{Trees Optimum Under Various Criteria}}},
  volume = {37},
  issn = {0036-1399},
  abstract = {In this paper we construct binary trees optimal under various criteria. In particular, we show that Hu-Tucker type algorithms can be used to find trees, whose leaves preserve a given order, that minimize certain sums of functions of path length, and also that minimize certain maximum functions of path length. The arguments are based on a new proof of the original Hu-Tucker algorithm.},
  number = {2},
  journal = {SIAM Journal on Applied Mathematics},
  author = {Hu, T. C. and Kleitman, Daniel J. and Tamaki, Jeanne K.},
  year = {1979},
  pages = {246-256}
}

@article{hu_optimum_,
  title = {Optimum {{Alphabetic Binary Trees}}},
  abstract = {We describe a modi cation of the Hu\{Tucker algorithm for constructing an optimal alphabetic tree that runs in O(n) time for several classes of inputs. These classes can be described in simple terms and can be detected in linear time. We also give simple conditions and a linear algorithm for determining, in some cases, if two adjacent nodes will be combined in the optimal alphabetic tree.\vphantom\}},
  language = {en},
  author = {Hu, T C and Morgenthaler, J D},
  pages = {10},
  file = {/Users/doisinkidney/Zotero/storage/JJBJDKAY/Hu and Morgenthaler - Optimum Alphabetic Binary Trees.pdf}
}


